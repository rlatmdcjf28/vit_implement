training_device: lab
dataset_dir: /home/ai/User/seungchul/cifar10/
epochs: 800
batch_size: 256
learning_rate: 0.0001
optimizer: Adam
act_fn: GeLU
device: cuda
dropout_rate: 0.0
random_seed: 123
model_summary_dir: model_summary.txt
result_dir: ./result_dir/20240821_2015/
classification_report_dir: classification_report.txt
now_epochs: 1
patch_size: 8
num_enc_layers: 24
mlp_size: 4096
hidden_dim: 1024
num_heads: 16
num_classes: 10
learning_scheduler: 

linear_lr = lr_scheduler.LinearLR(optim, total_iters=10)
step_lr = lr_scheduler.StepLR(optim, step_size=10, gamma=0.8)#
scheduler = lr_scheduler.SequentialLR(optim, 
                                          milestones=[10],
                                          schedulers = \
                                          [linear_lr, cawr])
                                          
